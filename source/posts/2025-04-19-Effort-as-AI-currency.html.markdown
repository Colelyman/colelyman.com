---
title: "Effort as AI Currency"
author: "Cole Lyman"
date: 2025-04-19
---

The other day I was reading Theodore Roosevelt's book [The Strenuous Life](https://www.google.com/books/edition/The_Strenuous_Life/YIpZAAAAYAAJ?hl=en&gbpv=1) and on page 4[^speech] he remarks that "Freedom from effort in the present merely means that there has been stored up effort in the past." When I read this, I couldn't help but think that the current Large Language Models (LLMs) are an example of this; effort has been stored up from the past and is brought to the present.

[^speech]: from a speech Roosevelt gave "before the Hamilton Club, Chicago, April 10, 1899"

LLMs only produce coherent output because they were trained on vast amounts of coherent data. This data[^internet] represents the cumulative public efforts of people online. The effectiveness of the model is entirely dependent on the quality of the training data. If the model was trained on randomly generated words, which requires low effort, the output of the model would be useless.

[^internet]: scraped from the internet, which obviously presents major ethical and legal considerations of how the LLMs were trained, for which [others](https://buttericklaw.com/) are much more qualified to speak on than myself.

I'm honestly not sure what to conclude from this thought, other than keep in mind that the time you "saved" using a LLM is due to the stored up effort of many people.
